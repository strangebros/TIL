## 개요

- 7장에서는 '최소 한 번' 전달에 초점을 맞춤 (메시지가 유실되지 않음이 보장됨)
- 하지만 여전히 메시지 중복의 가능성은 존재

### 문제가 되는 예시

- 특정 이벤트를 읽어 평균을 계산하고 결과를 산출하는 애플리케이션의 경우, 결과만 보는 입장에서는 어떤 이벤트가 두 번 처리되어 평균값이 잘못 계산된 사실을 알아내기 어려움
    - 즉, 이벤트가 중복 처리되어도 결과만으로는 오류를 쉽게 감지할 수 없음

### 카프카의 '정확히 한 번'

- `멱등적 프로듀서` & `트랜잭션`의 조합으로 이루어짐
    - 멱등적 프로듀서: 프로듀서 재시도로 인해 발생하는 중복을 방지
    - 트랜잭션: 스트림 처리 애플리케이션에서 '정확히 한 번' 처리를 보장

## 8.1 멱등적 프로듀서

### 멱등적

- 동일한 작업을 여러 번 실행해도 한 번 실행한 것과 결과가 같은 서비스

### 메시지 중복이 발생할 수 있는 '최소 한 번' 케이스

1. 파티션 리더가 프로듀서로부터 레코드를 받아 팔로워들에게 성공적으로 복제
2. 프로듀서에게 응답을 보내기 전에 파티션 리더가 있는 브로커에 크래시가 발생
3. 프로듀서는 응답을 받지 못해 타임아웃이 발생하고 메시지를 재전송
4. 재전송된 메시지가 새 리더에 도착하지만 이미 저장된 메시지이므로 중복이 발생

### 8.1.1 멱등적 프로듀서의 작동 원리

- 멱등적 프로듀서 기능을 켜면, 모든 메시지는 `고유한 프로듀서 ID` 와 `시퀀스 넘버`를 가짐
    - `대상 토픽 & 파티션 & 프로듀서 ID & 시퀸스 넘버` 를 합치면 각 메시지의 고유한 식별자가 됨
    - 각 브로커는 파티션에 쓰여진 마지막 5개 메시지들을 추적하기 위해 이 고유 식별자를 사용
    - 예전에 받은 적이 있는 메시지를 받게 될 경우, 에러를 발생시켜 중복 메시지를 거부
    - `max.in.flight.requests.per.connection`
        - 하나의 브로커 연결에서 응답을 받지 않은 채로 동시에 전송할 수 있는 최대 요청 수
        - 이 값이 5 이하여야, 파티션 별로 추적되어야 하는 시퀸스 넘버의 수를 제한할 수 있다.
- 만약 예상보다 높은 시퀀스 넘버를 받게 된다면?
    - e.g. 2번 다음 3번을 기대했는데, 27번 메시지를 받은 경우
    - 브로커는 `out of order sequence number` 에러를 발생시킴
    - 단, `트랜잭션` 없이 멱등적 프로듀서 기능만 사용할 것이라면 이 오류는 무시 가능
    - 설정은 잘 되어 있는지, 언클린 리더 선출이 발생되었는지 등을 확인할 필요가 있음

### 작동이 실패했을 경우, 멱등적 프로듀서가 어떻게 처리하는가?

1. 프로듀서 재시작 케이스
    1. 보통 새 프로듀서를 생성해 (직접 또는 쿠버네티스 환경 등) 장애가 난 프로듀서를 대체
    2. 멱등적 프로듀서 기능이 켜져있는 경우, 초기화 과정에서 브로커로 부터 프로듀서 ID 를 생성받음
        1. 트랜잭션 기능을 켜지 않은 경우, 프로듀서 초기화 시 마다 완전히 새로운 프로듀서 ID 를 생성
    3. 즉, 기존 프로듀서가 이미 전송한 메시지를 다시 전송해도 중복을 알아차릴 수 없음 (식별자가 달라지기 때문)
2. 브로커 장애 케이스
    1. 컨트롤러는 장애가 난 브로커가 리더를 맡고 있었던 파티션들에 대해 새 리더를 선출
        1. 프로듀서는 **메타데이터 프로토콜**을 통해 새로운 브로커가 리더임을 알아차리고 거기로 메시지를 쓰기 시작
    2. **하지만 새로운 리더 브로커 입장에서 어느 시퀀스 넘버까지 쓰여졌는지 알고, 중복 메시지를 걸러내는가?**
        1. 리더는 새 메시지가 쓰여질 때마다 인-메모리 프로듀서 상태에 저장된 초근 5개의 시퀸스 넘버를 업데이트
        2. 팔로워는 리더로부터 새로운 메시지를 복제할 때마다 자체적인 인-메모리 버퍼를 업데이트
        3. 즉, 팔로워가 리더가 된 시점에는 이미 메모리 안에 최근 5개의 시퀸스 넘버를 가지고 있게 되어, 아무런 지연 없이 새로운 메시지에 대한 유효성 검증이 재개될 수 있음
    3. 하지만, 갑자기 이전 리더가 다시 복구된다면?
        1. 인-메모리 프로듀서 상태는 더 이상 메모리에 없음
        2. 브로커는 복구에 도움이 될 수 있도록 종료되거나 새 세그먼트가 생성될 때마다 프로듀서 상태를 스냅샷 파일로 저장
        3. 브로커가 시작되면 스냅샷 + 현재 리더로부터 복제한 레코드를 사용해 최신 상태를 복구
            1. 브로커가 다시 리더가 될 수 있는 시점에는 최신 시퀀스 넘버를 갖게 됨
        4. 최신 스냅샷이 업데이트 되지 않더라도, 프로듀서 ID 와 시퀸스 넘버는 메시지에 저장되므로 파티션에 저장된 세그먼트들을 읽어와서 최신 상태로 복구가 가능함
    4. 만얀 메시지 보존 기간동안 들어온 메시지가 하나도 없다면?!
        1. 복구는 불가능하겠지만, 중복이 없다는 이야기이기도 하므로 그냥 신규 메시지부터 처리하면 됨 (프로듀서 상태가 없다는 경고메시지는 발생)

### 8.1.2 멱등적 프로듀서의 한계

- 프로듀서의 내부 로직 이슈로 인한 중복이 아닌, 프로듀서가 메시지 생성 자체를 두 번 하는 경우엔 중복을 방지할 수 없음
    - 프로듀서에서 직접 예외 처리를 하는 것 보단, 프로듀서에 탑재된 자체 재시도 매커니즘을 사용하라

### 8.1.3 멱등적 프로듀서 사용법

- 설정에 `enable.idempotence=true` 를 추가해주면 끝.
    - 이미 `acks=all` 설정이 잡혀있다면, 성능에는 차이가 없음
- 설정을 `true` 로 바꾸면 아래와 같은 변경이 발생
    - 프로듀서 ID 를 받아오기 위해 API 호출 추가 1건
    - 브로커들은 프로듀서 ID 를 통해 만들어진 시퀸스 넘버를 검증해, 메시지 중복을 방지
    - 장애가 발생해도, (위에서 논의한 과정을 통해) **메시지들의 순서가 보장**됨

## 8.2 트랜잭션

### 개요

- 트랜잭션 기능은, **카프카 스트림즈**를 사용해서 개발된 애플리케이션에 정확성을 보장하기 위해 도입
- 스트림 처리 애플리케이션이 정확한 결과를 산출하기 위해선, 각 레코드는 정확히 한 번만 처리되어야 하며 그 결과 역시 정확히 한번만 반영되어야 함
- 트랜잭션 기능은, 스트림 처리 애플리케이션의 기본 패턴인 `읽기-처리-쓰기` 패턴에서 사용하도록 개발됨
    - 이런 맥락에서 `정확히 한 번` 의미 구조를 보장

### 8.2.1 트랜잭션 활용 사례

1. 스트림 처리 로직에 집적/조인이 포함된 경우
    1. 결과 레코드의 오류 여부 판단이 훨씬 어려워짐
2. 금융 애플리케이션 등

### 8.2.2 트랜잭션이 해결하는 문제

> **하고 싶은 것**
원본 토픽에서 이벤트 읽기 → 처리 → 결과를 다른 토픽에 쓰기
> 
1. 애플리케이션 크래시로 인한 재처리
    1. 출력 토픽에는 이미 썼지만, 오프셋이 커밋되지 않은 경우 중복 발생 가능
2. 좀비 애플리케이션에 의해 발생하는 재처리
    1. 컨슈머가 카프카로부터 레코드 배치를 읽어온 직후 멈추거나, 연결이 끊어진다면?
    2. 컨슈머에 할당되어 있던 파티션들이 다른 컨슈머에 재할당(리밸런싱)되어, 레코드를 다시 읽고/처리하고/결과를 쓰게 됨
    3. 그 사이, `a` 에서 멈췄던 인스턴스가 다시 동작하게 되면, 마지막으로 읽어왔던 레코드를 처리하고 결과를 출력에 써버릴 수 있다 (중복 발생, 좀비)
        1. 새로운 레코드를 위해 폴링하거나, 하트비트를 보내서 다른 컨슈머가 자신이 맡은 파티션을 할당받은 상태임을 알아차리기 전까지 이럴 수 있음
    
    ### 8.2.3 트랜잭션은 어떻게 ‘정확히 한 번’ 을 보장하는가?
    
    > ‘정확히 한 번’ 은 이러한 읽기, 처리, 쓰기 작업이 원자적으로 이루어진다는 의미
    > 
    - 카프카 트랜잭션은 `원자적 다수 파티션 쓰기` 기능을 도입
        - 오프셋 커밋과 결과를 쓰는 것이 결국 둘 다 파티션에 메시지를 쓰는 과정을 수반한다는 점에 착안
        - (결과는 출력 토픽, 오프셋은 `_consumer_offsets` 토픽에)
    
    ![image.png](attachment:a8cd7650-c9e7-4602-88e3-cb77ed46674f:image.png)
    
    - 이를 위해선, `트랜잭션적 프로듀서` 를 사용해야 함
        - [`trasactional.id`](http://trasactional.id) 설정이 잡혀있고, `initTransactions()` 를 호출해서 초기화했다는 점이 일반 프로듀서와 다르다
        - [`trasactional.id`](http://trasactional.id) 는 재시작을 하더라도 값이 유지됨
        - 브로커는 [`trasactional.id`](http://trasactional.id) ↔ [`producer.id`](http://producer.id) 의 대응 관계를 유지하고 있다가, 프로듀서가 `initTransactions()` 를 호출하면 이전의 [`trasactional.id`](http://trasactional.id) 를 반환해준다
    - `좀비 펜싱` 을 통해 좀비 인스턴스가 중복 프로듀서를 생성하는 것을 방지할 수 있음
        - 일반적으로 `에포크(epoch)` 를 사용하는 방식이 쓰임
        - `initTransactions()` 호출 시 [`transactional.id`](http://transactional.id) 에 해당하는 에포크 값을 증가
        - 같은 [`trasactional.id`](http://trasactional.id) 를 갖지만 낮은 에포크 값을 가진 프로듀서가 메시지 전송 등을 요청하면 `FencedProducer` 에러가 발생하며 거부됨 (즉, 좀비가 중복 레코드를 쓰는 것은 불가능함)
        - 아파치 카프카 2.5 이후부터는 트랜잭션 메타데이터에 컨슈머 그룹 메타데이터를 추가할 수 있게 되어, 서로 다른 트랜잭션 ID 를 갖는 프로듀서들이 같은 파티션들에 레코드를 쓸 수 있게 됨
            
            ## 배경: 기존의 문제점
            
            ### Kafka 2.5 이전의 상황
            
            - **트랜잭션 ID 고유성 문제**: 각 프로듀서는 고유한 트랜잭션 ID가 필요했습니다
            - **펜싱(Fencing) 제약**: 같은 트랜잭션 ID를 가진 이전 프로듀서(좀비)를 차단하면, 새로운 프로듀서도 같은 ID를 사용해야 했습니다
            - **확장성 제한**: 여러 프로듀서가 동시에 같은 파티션에 쓰기 어려웠습니다
            
            ## Kafka 2.5의 개선: 컨슈머 그룹 메타데이터 추가
            
            ### 1. 새로운 메타데이터 구조
            
            ```
            트랜잭션 메타데이터 = {
                트랜잭션 ID,
                프로듀서 ID,
                프로듀서 Epoch,
                + 컨슈머 그룹 메타데이터 (새로 추가!)
            }
            
            ```
            
            ### 2. 펜싱 메커니즘의 진화
            
            **기존 펜싱 방식:**
            
            - 트랜잭션 ID만으로 좀비 프로듀서를 식별
            - 같은 트랜잭션 ID = 무조건 펜싱 대상
            
            **개선된 펜싱 방식:**
            
            - 트랜잭션 ID + 컨슈머 그룹 정보로 더 정밀한 식별
            - 컨슈머 그룹의 세대(generation)와 멤버 정보를 함께 고려
            
            ### 3. 실제 동작 예시
            
            ```
            시나리오: Kafka Streams 애플리케이션에서 3개의 인스턴스가 동작
            
            [인스턴스 A] - 트랜잭션 ID: app-0
            [인스턴스 B] - 트랜잭션 ID: app-1
            [인스턴스 C] - 트랜잭션 ID: app-2
            
            모두 같은 파티션들(P1, P2, P3)에 접근 가능
            
            ```
            
            **Kafka 2.5 이전:**
            
            - 인스턴스 A가 좀비가 되면, app-0 ID를 가진 모든 작업이 차단됨
            - 다른 인스턴스가 P1에 쓰려면 리밸런싱 완료까지 대기
            
            **Kafka 2.5 이후:**
            
            - 컨슈머 그룹 메타데이터로 정확한 좀비 식별
            - 인스턴스 A만 펜싱, B와 C는 즉시 P1에 쓰기 가능
            - 각자의 트랜잭션 ID로 독립적 작업 수행
            
            ## 핵심 이점
            
            ### 1. **병렬 처리 향상**
            
            여러 프로듀서가 서로 다른 트랜잭션 ID로 같은 파티션에 동시 쓰기 가능
            
            ### 2. **더 정밀한 좀비 탐지**
            
            컨슈머 그룹 정보를 통해 실제 좀비만 정확히 식별하고 차단
            
            ### 3. **빠른 복구**
            
            좀비 펜싱 중에도 정상 프로듀서들은 작업 계속 가능
            
            ### 4. **Kafka Streams 최적화**
            
            스트림 처리 애플리케이션의 리밸런싱 시 다운타임 최소화
            
            ## 실무 적용 예
            
            ```java
            // Kafka Streams 설정 예시
            Properties props = new Properties();
            props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,
                      StreamsConfig.EXACTLY_ONCE_V2); // 2.5+ 버전 사용
            props.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-app");
            
            // 이제 여러 스트림 인스턴스가 효율적으로 협업 가능
            ```
            
    - 컨슈머 레벨의 격리 수준 설정
        - a.k.a 읽어오는 쪽의 설정도 중요하다.
        - `isolation.level` 을 통해 트랜잭션을 써서 쓰여진 메시지들을 읽어오는 방식 제거 가능
        - `read_committed` 인 경우, `poll()` 시 `커밋된 트랜잭션 || 트랜잭션에 속하지 않는 메시지`만 리턴
            - 단, 이 경우에도 트랜잭션에 속한 모든 메시지가 리턴됨이 보장되는 것은 아님
                - 트랜잭션에 구독하지 않은 여러 토픽에 속할 수도 있기 때문
            - 또한, 어느 메시지가 어느 트랜잭션에 속하는지에 대해서도 컨슈머는 알 수 없음
            - 이 모드에서는, 아직 진행 중인 트랜잭션이 처음으로 시작된 시점 (`LSO`) 이후에 쓰여진 메시지는 리턴되지 않음
                - 트랜잭션이 커밋/중단되거나 [`transaction.timeout.ms`](http://transaction.timeout.ms) 를 넘어가 중단될 때 까지 보류됨
        - `read_uncommitted` (기본값) 설정 시 진행 중이거나 중단된 트랜잭션에 속하는 메시지도 포함됨
        
        ![image.png](attachment:b83b9fab-5c4c-4d65-baa2-254c9027467b:image.png)
        

### 8.2.4 트랜잭션으로 해결할 수 없는 문제들

1. 스트림 처리에 있어서의 부수 효과
    1. 스트림 처리의 결과로 `이메일 발송, API 호출, 파일 쓰기` 등의 작업이 이뤄질 경우, 당연하게도 이는 `정확히 한 번` 을 보장하지 않는다.
    2. `정확히 한 번` 은 카프카에 쓰여지는 메시지에 대해서만 보장된다
2. 카프카 토픽에서 읽어서 DB 에 쓰는 경우
    1. 결과를 쓸 때 프로듀서가 사용되지 않으므로, 결과를 쓸 때 카프카 트랜잭션의 도움을 받을 수 없다
    2. DB 트랜잭션 레벨의 도움을 받아 오프셋 & 결과를 모두 db 에 쓰는 방법은 가능
        1. `outbox pattern` (뒤에서 계속,,)
3. DB 에서 읽어서, 카프카에 쓰고, 여기서 다시 다른 DB 에 쓰는 경우 (전 구간에서 트랜잭션을 유지하고 싶은 경우)
    1. 카프카는 이러한 종류의 `종단 보장` 을 지원하지 않는다
    2. `read_committed` 보장은 DB 트랜잭션을 보존하기엔 너무 약하다
        1. 컨슈머는 아직 커밋되지 않은 레코드를 볼 수 없다 → 보장
        2. 이전에 이미 커밋된 트랜잭션의 레코드를 모두 봤을 것이다 → 보장 X
            1. 일부 토픽에서 랙이 발생했을 수도 있기 때문
        3. 트랜잭션의 경계를 파악할 수도 없기 때문에, 트랜잭션의 시작/종료 등을 파악할 수 없음
4. 한 클러스터에서 다른 클러스터로 데이터 복제
    1. 하나의 카프카 클러스터에서 다른 클러스터로 복사할 때 `정확히 한 번` 은 가능 (미러메이커 2.0)
    2. 다만, 트랜잭션의 원자성은 보장하지 못함
        1. 트랜잭션의 모든 데이터를 읽어왔는지는 알 수도 없고, 보장할 수도 없음
        2. (토픽의 일부만 구독한 경우 등)
5. 발행/구독 패턴
    1. `read_committed` 로 설정되어 있으면, 중단된 트랜잭션에 속한 레코드를 보지는 못하지만,
    2. 오프셋 커밋 로직에 따라 여전히 컨슈머들은 메시지를 한 번 이상 `처리` 할 수 있다
    3. JMS 트랜잭션과의 차이점은, `read_committed` 설정이 되어 있어야 커밋되지 않은 메시지를 주지 않는 다는 것.
        1. JMS 는 기본적으로 커밋되지 않은 메시지를 주지 않음
    - cluade의 첨언
        
        ### 1. **"정확히 한 번(Exactly-Once)" 처리의 한계**
        
        - 읽기-처리-쓰기 패턴과 달리, 발행/구독 패턴에서는 완전한 "정확히 한 번" 보장이 어렵다는 점을 지적합니다
        - 트랜잭션을 사용해도 여전히 중복 처리 가능성이 존재합니다
        
        ### 2. **read_committed 모드의 역할**
        
        - `read_committed` 모드로 설정된 컨슈머는 중단(abort)된 트랜잭션의 메시지를 볼 수 없습니다
        - 이는 데이터 일관성을 위한 중요한 보장입니다
        - 하지만 이것만으로는 "정확히 한 번" 처리를 완전히 보장할 수 없습니다
        
        ### 3. **오프셋 커밋의 문제**
        
        - 컨슈머의 오프셋 커밋 로직에 따라 메시지가 **한 번 이상** 처리될 수 있습니다
        - 예: 메시지 처리 후 오프셋 커밋 전에 장애가 발생하면, 재시작 시 같은 메시지를 다시 처리하게 됩니다
        
        ### 4. **JMS와의 비교**
        
        - Kafka의 트랜잭션 보장은 JMS(Java Message Service)와 유사하지만 차이점이 있습니다
        - **Kafka**: 컨슈머가 `read_committed`로 설정되어야만 커밋되지 않은 트랜잭션을 숨깁니다
        - **JMS**: 모든 컨슈머에게 자동으로 커밋되지 않은 트랜잭션을 숨깁니다

### 8.2.5 트랜잭션 사용법

- 가장 일반적인 방법
    - 카프카 스트림즈에서 `exactly-once` 보장을 활성화
    - `processing.guarantee` 설정을 `exactly_once` 로 설정
- 카프카 스트림즈 대신 트랜잭션 API 를 직접 사용하는 방법
    
    ```java
    Properties producerProps = new Properties();
    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, "DemoProducer");
    producerProps.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);  // ① transactional.id 설정
    
    producer = new KafkaProducer<>(producerProps);
    
    Properties consumerProps = new Properties();
    consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");  // ② 자동 커밋 비활성화
    consumerProps.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");  // ③ 커밋된 트랜잭션만 읽기
    
    consumer = new KafkaConsumer<>(consumerProps);
    
    producer.initTransactions();  // ④ 트랜잭션 초기화
    
    consumer.subscribe(Collections.singleton(inputTopic));  // ⑤ 입력 토픽 구독
    
    while (true) {
        try {
            ConsumerRecords<Integer, String> records = 
                consumer.poll(Duration.ofMillis(200));
            if (records.count() > 0) {
                producer.beginTransaction();  // ⑥ 트랜잭션 시작
                for (ConsumerRecord<Integer, String> record : records) {
                    ProducerRecord<Integer, String> customizedRecord = 
                        transform(record);  // ⑦ 레코드 변환
                    producer.send(customizedRecord);
                }
                Map<TopicPartition, OffsetAndMetadata> offsets = consumerOffsets();
                producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());  // ⑧ 오프셋을 트랜잭션에 포함
                producer.commitTransaction();  // ⑨ 트랜잭션 커밋
            }
        } catch (ProducerFencedException|InvalidProducerEpochException e) {  // ⑩ Fencing 예외 처리
            throw new KafkaException(String.format(
                "The transactional.id %s is used by another process", transactionalId));
        } catch (KafkaException e) {
            producer.abortTransaction();  // ⑪ 트랜잭션 중단
            resetToLastCommittedPositions(consumer);
        }
    }
    ```
    
    1. **transactional.id**: 파티션별 원자적 쓰기를 보장하는 고유 ID. 애플리케이션 인스턴스를 식별
    2. **자동 커밋 비활성화**: 트랜잭션 내에서 프로듀서가 오프셋을 직접 관리하므로 비활성화 필수
    3. **read_committed**: 완료된 트랜잭션만 읽어 데이터 일관성 보장. 진행 중이거나 실패한 트랜잭션은 무시
    4. **트랜잭션 초기화**: 트랜잭션 ID 등록, 에포크 값 증가로 좀비 프로세스 방지, 진행 중인 트랜잭션 중단
    5. **동적 파티션 할당**: KIP-447 이후 subscribe API 사용 가능. 컨슈머 그룹 정보를 활용한 좀비 펜싱 지원
    6. **트랜잭션 시작**: 호출 시점부터 커밋/중단까지 모든 작업이 하나의 원자적 단위로 처리
    7. **레코드 변환**: 실제 비즈니스 로직 수행 부분
    8. **오프셋 커밋**: 트랜잭션 내에서만 오프셋 커밋. 결과 쓰기 실패 시 오프셋도 롤백되어 재처리 가능
    9. **트랜잭션 커밋**: 모든 메시지와 오프셋 쓰기 완료. 성공 시 다음 배치 처리 진행
    10. **ProducerFencedException**: 현재 프로듀서가 좀비가 된 경우. 같은 ID의 새 인스턴스가 실행 중이므로 즉시 종료
    11. **예외 처리**: 트랜잭션 중단, 컨슈머 위치 리셋 후 재시도

### 8.2.6 트랜잭션 ID 와 펜싱

- 프로듀서가 사용할 트랜잭션 ID 를 선택하는 것은, 중요하면서도 어려움
    - 트랜잭션 ID 를 잘못 할당해주면, 애플리케이션에 에러가 발생하거나 `정확히 한 번` 보장을 준수할 수 없게 됨
- **요구사항: 트랜잭션 ID 가, 동일 애플리케이션에 대해서는 일관적으로 유지되면서 서로 다른 애플리케이션에 대해서는 다름이 보장된다**
    - 버전 2.5까지는 트랜잭션 ID 가 랜덤하게 설정됨
        - 즉, 동일한 파티션에 쓰기 작업을 할 때 언제나 동일한 트랜잭션 ID 가 쓰일거라는 보장이 없음
- 2.5 에서 소개된 KIP-447 은 펜싱을 수행하는 두 번째 방법, 즉, 트랜잭션 ID 와 컨슈머 그룹 메타데이터를 함께 사용하는 펜싱을 도입
    - 프로듀서의 오프셋을 커밋 할 때 단순한 컨슈머 그룹 ID 가 아닌, 컨슈머 그룹 메타데이터를 인수로 전달
- 예제 케이스
    
    ![image.png](attachment:ab08f8d5-6608-4c52-9d99-7f40809486de:image.png)
    
    ![image.png](attachment:f163f8ea-0bfe-4f3e-bdeb-125ac4f9fc40:image.png)
    
    - `그림 8-4` 와 같이, 컨슈머 A와 프로듀서 A 가 포함된 애플리케이션 인스턴스가 좀비가 되고, 컨슈머 B 가 두 파티션으로 부터 레코드를 모두 처리하기 시작
    - 좀비가 파티션 0 에 쓰기를 막고 싶다면, 기존에는 다른 어떠한 프로듀서 ID 도 쓰기 작업을 할 수 없도록 막았어야 함
        - 그리고, 프로듀서 ID 가 `A` 인 새 프로듀서를 생성해야 함 → 낭비!
    - 대신, 트랜잭션에 컨슈머 그룹 정보를 포함
        - 그러면 프로듀서 B 로의 트랜잭션은 명백하게 다음 세대의 컨슈머 그룹에서 왔으므로 작업을 진행할 수 있다 (리밸런싱이 일어나면 컨슈머 그룹 세대가 증가)
        - 좀비인 프로듀서 A 로 부터의 트랜잭션은 이전 세대의 컨슈머 그룹에서 왔으므로 펜싱됨

### 8.2.7 트랜잭션 작동 원리

- [ ]  완성
- `1. 2단계 커밋`, `2. 트랜잭션 로그` 를 사용
    1. 현재 진행 중인 트랜잭션이 존재함을 `로그`에 기록. 연관된 파티션도 함께 기록
    2. 로그에 커밋 / 중단 시도 기록
        1. 일단 로그에 기록이 남으면 언젠가는 커밋되거나 중단되어야 함
    3. 모든 파티션에 트랜잭션 마커를 씀
    4. 트랜잭션이 종료되었음을 로그에 씀
- 위 알고리즘 구현을 위해 `_transaction_state` 내부 토픽에 트랜잭션 로그를 쌓음
- 트랜잭션 API 를 사용하는 코드 흐름에 따른 알고리즘 동작 확인
    1. 프로듀서의 `initTransaction()` 호출
        1. 트랜잭션 프로듀서의 트랜잭선 코디네이터 역할을 맡을 브로커로 보내짐
        2. 브로커는 전체 프로듀서의 트랜잭션 코디네이터 역할을 나눠 맡음

## 8.3 트랜잭션 성능

- 트랜잭션은 프로듀서에 약간의 오버헤드를 발생시킴
    - 트랜잭션 ID 등록, 파티션 등록 요청은 최대 한번씩만 발생
    - 단, 트랜잭션 초기화 / 커밋 요청은 동기적으로 작동하여, 완료/실패 시 까지 어떤 데이터도 전송되지 않아 오버헤드 증가
    - 트랜잭션에 포함된 메시지 수와는 무관하므로, 트랜잭션에 많은 수의 메시지를 넣을 수록 처리량 증가
        
        ## Kafka 트랜잭션 오버헤드가 메시지 수와 무관한 이유
        
        ### 1. **Kafka 트랜잭션 아키텍처 이해**
        
        Kafka는 **Two-Phase Commit (2PC) 프로토콜**의 변형을 사용합니다. 핵심 컴포넌트는:
        
        - **Transaction Coordinator**: 트랜잭션 상태를 관리하는 브로커 내 컴포넌트
        - **Transaction Log (`__transaction_state` 토픽)**: 트랜잭션 메타데이터 저장
        - **Control Messages**: 트랜잭션 경계를 표시하는 특수 메시지
        
        ### 2. **트랜잭션 라이프사이클과 오버헤드 분석**
        
        ```java
        // 트랜잭션 시작
        producer.beginTransaction();  // O(1) - Coordinator에 단일 RPC
        
        // 메시지 전송 - 트랜잭션 오버헤드 없음
        for (int i = 0; i < 10000; i++) {
            producer.send(record);  // 일반 전송과 동일, 단지 PID/Epoch 헤더만 추가
        }
        
        // 트랜잭션 커밋
        producer.commitTransaction();  // O(1) - 고정 오버헤드
        
        ```
        
        ### **Phase 1: Initialization (고정 비용)**
        
        - Producer ID (PID) 할당: 1회 RPC
        - Epoch 초기화: Fencing을 위한 버전 관리
        - 시간 복잡도: **O(1)**
        
        ### **Phase 2: Message Production (가변 비용이지만 트랜잭션과 무관)**
        
        ```
        각 메시지는 다음 헤더만 추가:
        - PID (8 bytes)
        - Epoch (2 bytes)
        - Sequence Number (4 bytes)
        
        → 메시지당 약 14 bytes 오버헤드 (트랜잭션 여부와 무관)
        
        ```
        
        ### **Phase 3: Commit Protocol (고정 비용)**
        
        ```
        1. WriteTxnMarkers 요청 → Coordinator
        2. Coordinator → 각 Partition Leader에 Control Message 전송
        3. Transaction Log에 COMMITTED 상태 기록
        
        → 파티션 수에 비례하지만, 메시지 수와는 무관
        
        ```
        
        ### 3. **왜 메시지 수와 무관한가?**
        
        **핵심 원리: Lazy Evaluation + Batch Processing**
        
        ```
        전통적 DB 트랜잭션:
        BEGIN → INSERT(검증) → INSERT(검증) → ... → COMMIT
                ↑ 각 연산마다 로그 기록
        
        Kafka 트랜잭션:
        BEGIN → SEND → SEND → ... → COMMIT
                ↑ 메시지는 일반 배치로 전송, 검증은 Consumer 시점에 지연
        
        ```
        
        **트랜잭션 마커 메커니즘:**
        
        - 모든 메시지 후 **단 하나의 Control Message**만 각 파티션에 append
        - Consumer는 이 마커를 만날 때까지 메시지를 버퍼링
        - 마커 확인 후 일괄 처리 or 폐기 결정
        
        ### 4. **성능 특성 분석**
        
        ```
        트랜잭션 오버헤드 = Tinit + Tcommit + (P × Tmarker)
        
        여기서:
        - Tinit: 초기화 시간 (약 1-2ms)
        - Tcommit: 커밋 프로토콜 시간 (약 5-10ms)
        - P: 관련 파티션 수
        - Tmarker: 마커 전송 시간 (약 1ms/partition)
        
        주목: 메시지 수(N)가 수식에 없음!
        
        ```
        
        ### 5. **최적화 전략**
        
        **Idempotent Producer와의 시너지:**
        
        ```java
        props.put("enable.idempotence", "true");
        // Sequence Number를 통한 중복 제거는 트랜잭션과 독립적으로 동작
        // 트랜잭션 오버헤드 추가 없이 Exactly-Once 보장
        
        ```
        
        **배치 크기 최적화:**
        
        ```java
        // 좋은 패턴 - 트랜잭션당 많은 메시지
        producer.beginTransaction();
        records.forEach(producer::send);  // 1000개 메시지
        producer.commitTransaction();      // 오버헤드 1회
        
        // 나쁜 패턴 - 메시지당 트랜잭션
        records.forEach(record -> {
            producer.beginTransaction();   // 오버헤드 1000회!
            producer.send(record);
            producer.commitTransaction();
        });
        
        ```
        
        ### 6. **내부 구현 디테일**
        
        **LSO (Last Stable Offset) 메커니즘:**
        
        - Consumer는 LSO까지만 읽기 가능
        - 트랜잭션 중인 메시지는 LSO 이후에 위치
        - Commit 시 LSO가 전진 → 모든 메시지가 한 번에 visible
        
        이는 **MVCC (Multi-Version Concurrency Control)**와 유사한 방식으로, 메시지 수와 무관하게 가시성을 제어합니다.
        
        **결론적으로**, Kafka는 트랜잭션 경계를 Control Message로 표시하고, 실제 데이터는 일반 배치 전송을 사용하여, 트랜잭션 오버헤드를 메시지 수와 분리시켰습니다. 이는 대용량 스트리밍 환경에 최적화된 설계입니다.
        
- 컨슈머는, 커밋 마커를 읽어오는 작업에 관련해서 약간의 오버헤드 존재
    - 또한, `read_committed` 일 때는 완료되지 않은 트랜잭션 레코드들이 리턴되지 않으므로 종단 지연이 길어짐
    - 다만 브로커들은 해당 메시지들을 아예 보내지 않기 때문에, 컨슈머에서 별도로 처리할 일은 없음 (이로 인한 처리량 감소는 없음)